{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python torch pybase64 numpy supervision matplotlib pytesseract sam2 pillow imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
    "%cd ./segment-anything-2\n",
    "!pip install -e . -q\n",
    "\n",
    "!mkdir -p ./checkpoints\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P ./checkpoints\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P ./checkpoints\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P ./checkpoints\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./segment-anything-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import base64\n",
    "\n",
    "import math\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures torch\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets Device to CUDA GPU if available and  configures the SAM2 model\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT = f\"./checkpoints/sam2_hiera_large.pt\"\n",
    "CONFIG = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the segmented output\n",
    "IMAGE_PATH = \"/home/d4rkc10ud/Documents/Projects/SmartSplit/receipt_scan/inputs/receipt_walmart.png\"\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam2_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotates and displays the source and segmented images\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "detections = sv.Detections.from_sam(sam_result=sam2_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crops the images based on segment and identifies and saves the segmented receipt image\n",
    "cropped_images = []\n",
    "area = math.inf\n",
    "image_num = None\n",
    "\n",
    "for i, result in enumerate(sam2_result):\n",
    "    print(f\"Result #{i} bbox:\", result[\"bbox\"])\n",
    "    x, y, width, height = result[\"bbox\"]\n",
    "    \n",
    "    x_end = x + width\n",
    "    y_end = y + height\n",
    "    \n",
    "    cropped_image = image_rgb[math.floor(y):math.ceil(y_end), math.floor(x):math.ceil(x_end)]\n",
    "    \n",
    "    crop_text = pytesseract.image_to_string(cropped_image).lower()\n",
    "    \n",
    "    if result[\"area\"] < area and (\"total\" in crop_text or \"receipt\" in crop_text):\n",
    "        area = result[\"area\"]\n",
    "        image_num = i\n",
    "        \n",
    "    cropped_images.append(cropped_image)\n",
    "\n",
    "    cv2.imwrite(f\"/home/d4rkc10ud/Documents/Projects/SmartSplit/receipt_scan/cropped_receipt/cropped_image_{i}.png\", cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "if image_num != None:\n",
    "    x, y, width, height = sam2_result[image_num][\"bbox\"]\n",
    "    x_end = x + width\n",
    "    y_end = y + height\n",
    "    cropped_image = image_rgb[math.floor(y):math.ceil(y_end), math.floor(x):math.ceil(x_end)]\n",
    "    cv2.imwrite(\"/home/d4rkc10ud/Documents/Projects/SmartSplit/receipt_scan/cropped_receipt/cropped_image.png\", cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n",
    "else:\n",
    "    print(\"Receipt Crop not found\")\n",
    "\n",
    "for i, cropped_image in enumerate(cropped_images):\n",
    "    plt.subplot(1, len(cropped_images), i + 1)\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines preprocessing function\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imutils\n",
    "\n",
    "\n",
    "def preprocess_receipt(image):\n",
    "    \n",
    "    processed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    dist = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "    dist = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    dist = (dist * 255).astype(\"uint8\")\n",
    "    processed = cv2.threshold(dist, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the cropped image and preprocesses it for OCR detection\n",
    "IMAGE_PATH = \"/home/d4rkc10ud/Documents/Projects/SmartSplit/receipt_scan/cropped_receipt/cropped_image.png\"\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "preprocessed_image = preprocess_receipt(image)\n",
    "\n",
    "SAVE_PATH = \"/home/d4rkc10ud/Documents/Projects/SmartSplit/receipt_scan/cropped_receipt/preprocessed_image.png\"\n",
    "cv2.imwrite(SAVE_PATH, preprocessed_image)\n",
    "# cv2.imshow(\"Preprocessed Image\", preprocessed_image)\n",
    "\n",
    "plt.imshow(preprocessed_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SmartSplit-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
